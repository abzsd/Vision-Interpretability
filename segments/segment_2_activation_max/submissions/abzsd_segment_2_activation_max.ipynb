{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment 2: Activation Maximization\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](#)\n",
    "\n",
    "**In Segment 1**, we fed images into a CNN and asked: *\"What does each neuron respond to?\"*\n",
    "We saw feature maps light up for edges, textures, and shapes.\n",
    "\n",
    "Now we flip the question entirely:\n",
    "\n",
    "> **What input image would make a specific neuron fire as hard as possible?**\n",
    "\n",
    "This is **activation maximization** — and the answers are surprisingly beautiful.\n",
    "\n",
    "**What you will learn:**\n",
    "1. What activation-maximized images look like for real neurons inside InceptionV1\n",
    "2. How gradient ascent on *pixels* (not weights) generates these images\n",
    "3. Why image parameterization is the difference between noise and clarity\n",
    "4. How this technique reveals the full feature hierarchy of a network\n",
    "\n",
    "**Tools:** PyTorch, [Lucent](https://github.com/greentfrapp/lucent) library, InceptionV1\n",
    "\n",
    "Let's start with the payoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Setup ----------\n\n",
    "# Lucent is the PyTorch port of Google's Lucid interpretability library.\n\n",
    "# Uncomment the line below if running in Colab:\n\n",
    "# !pip install torch-lucent\n\n",
    "\n\n",
    "import torch\n\n",
    "import numpy as np\n\n",
    "import matplotlib.pyplot as plt\n\n",
    "\n\n",
    "from lucent.modelzoo import inceptionv1\n\n",
    "from lucent.optvis import render, param, transform, objectives\n\n",
    "\n\n",
    "# Load pretrained InceptionV1 (trained on ImageNet)\n\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n",
    "model = inceptionv1(pretrained=True).to(device).eval()\n\n",
    "\n\n",
    "print(f\"Device: {device}\")\n\n",
    "print(f\"Model: InceptionV1 (pretrained on ImageNet)\")\n\n",
    "print(f\"Target layer: mixed4a — a mid-depth Inception module with rich, interpretable features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- The Payoff: what do 10 neurons \"dream\" about? ----------\n\n",
    "# Each image below answers: \"What input would make this neuron happiest?\"\n\n",
    "\n\n",
    "LAYER = \"mixed4a\"\n\n",
    "NUM_NEURONS = 10\n\n",
    "\n\n",
    "images = []\n\n",
    "for i in range(NUM_NEURONS):\n\n",
    "    # objectives.channel targets one channel (neuron) in the named layer\n\n",
    "    # render_vis optimizes a random image to maximize that neuron's activation\n\n",
    "    imgs = render.render_vis(\n\n",
    "        model,\n\n",
    "        objectives.channel(LAYER, i),\n\n",
    "        param_f=lambda: param.image(128, fft=True, decorrelate=True),\n\n",
    "        thresholds=(512,),\n\n",
    "        show_inline=False,\n\n",
    "        show_image=False,\n\n",
    "    )\n\n",
    "    images.append(imgs[0][0])   # extract the H x W x 3 numpy array\n\n",
    "    print(f\"  Neuron {i} done\")\n\n",
    "\n\n",
    "# --- Display as a 2x5 gallery ---\n\n",
    "fig, axes = plt.subplots(2, 5, figsize=(16, 7))\n\n",
    "for i, ax in enumerate(axes.flat):\n\n",
    "    ax.imshow(images[i])\n\n",
    "    ax.set_title(f\"mixed4a : {i}\", fontsize=11, fontweight=\"bold\")\n\n",
    "    ax.axis(\"off\")\n\n",
    "\n\n",
    "fig.suptitle(\n\n",
    "    '10 Neurons, 10 Different \"Dreams\" — each neuron has learned to detect a unique pattern',\n\n",
    "    fontsize=13, fontweight=\"bold\",\n\n",
    ")\n\n",
    "plt.tight_layout()\n\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What just happened?\n",
    "\n",
    "Each image above was **not drawn by a human** and **not taken by a camera**. It was generated by an optimization process:\n",
    "\n",
    "1. Start with a random noise image\n",
    "2. Feed it through the frozen network\n",
    "3. Measure how strongly the target neuron activates\n",
    "4. Compute the gradient of that activation with respect to the input pixels\n",
    "5. Nudge the pixels in the direction that *increases* the activation (gradient ascent)\n",
    "6. Repeat for hundreds of steps\n",
    "\n",
    "The result reveals the **visual pattern that neuron has learned to detect** — textures, curves, grids, color combinations, or parts of objects.\n",
    "\n",
    "Take a moment to look at the gallery above. Some images might resemble fur, honeycombs, spirals, or woven fabric. Each one is a different feature detector that InceptionV1 discovered from millions of training images.\n",
    "\n",
    "**But how does this optimization actually work under the hood?** Let's build it from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- DIY: activation maximization from scratch ----------\n\n",
    "# We'll implement the core algorithm in raw PyTorch (no Lucent)\n\n",
    "# to understand exactly what's happening.\n\n",
    "\n\n",
    "TARGET_NEURON = 0  # same neuron we visualized above in the gallery\n\n",
    "\n\n",
    "# Step 1: Start from random noise — requires_grad=True because we optimize pixels\n\n",
    "input_img = torch.randn(1, 3, 224, 224, device=device, requires_grad=True)\n\n",
    "optimizer = torch.optim.Adam([input_img], lr=0.05)\n\n",
    "\n\n",
    "# Step 2: Hook into mixed4a to read its output during forward pass\n\n",
    "activation_store = {}\n\n",
    "def hook_fn(module, inp, out):\n\n",
    "    activation_store[\"value\"] = out\n\n",
    "\n\n",
    "hook_handle = dict(model.named_modules())[\"mixed4a\"].register_forward_hook(hook_fn)\n\n",
    "\n\n",
    "# Step 3: Gradient ascent — maximize the target channel's mean activation\n\n",
    "NUM_STEPS = 256\n\n",
    "act_history = []           # track activation value at each step\n\n",
    "snapshots = []             # save the image at key moments\n\n",
    "snapshot_steps = {0, 32, 64, 128, 255}\n\n",
    "\n\n",
    "for step in range(NUM_STEPS):\n\n",
    "    optimizer.zero_grad()\n\n",
    "    model(input_img)\n\n",
    "\n\n",
    "    # Mean activation of the target channel across all spatial positions\n\n",
    "    act_val = activation_store[\"value\"][0, TARGET_NEURON].mean()\n\n",
    "    (-act_val).backward()  # negate because Adam minimizes, but we want to maximize\n\n",
    "    optimizer.step()\n\n",
    "\n\n",
    "    act_history.append(act_val.item())\n\n",
    "\n\n",
    "    if step in snapshot_steps:\n\n",
    "        snap = input_img[0].detach().cpu().permute(1, 2, 0).numpy()\n\n",
    "        snap = (snap - snap.min()) / (snap.max() - snap.min() + 1e-8)\n\n",
    "        snapshots.append((step, snap.copy()))\n\n",
    "\n\n",
    "hook_handle.remove()\n\n",
    "\n\n",
    "# --- Display: filmstrip of snapshots + activation curve ---\n\n",
    "fig = plt.figure(figsize=(16, 4))\n\n",
    "\n\n",
    "for i, (step, snap) in enumerate(snapshots):\n\n",
    "    ax = fig.add_subplot(1, len(snapshots) + 1, i + 1)\n\n",
    "    ax.imshow(snap)\n\n",
    "    ax.set_title(f\"Step {step}\", fontsize=9)\n\n",
    "    ax.axis(\"off\")\n\n",
    "\n\n",
    "ax_curve = fig.add_subplot(1, len(snapshots) + 1, len(snapshots) + 1)\n\n",
    "ax_curve.plot(act_history, color=\"steelblue\", linewidth=1.5)\n\n",
    "ax_curve.set_xlabel(\"Step\")\n\n",
    "ax_curve.set_ylabel(\"Activation\")\n\n",
    "ax_curve.set_title(\"Neuron activation\\nover optimization\", fontsize=9)\n\n",
    "\n\n",
    "fig.suptitle(\n\n",
    "    f\"DIY gradient ascent on mixed4a neuron {TARGET_NEURON} — the pattern emerges, but it's noisy\",\n\n",
    "    fontsize=12, fontweight=\"bold\",\n\n",
    ")\n\n",
    "plt.tight_layout()\n\n",
    "plt.show()\n\n",
    "\n\n",
    "print(\"The structure is there, but the image is full of high-frequency noise.\")\n\n",
    "print(\"This is where parameterization tricks make all the difference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why so noisy?\n",
    "\n",
    "Our DIY version optimized **raw pixel values** directly. The optimizer found high-frequency patterns that technically increase the neuron's activation but look like static to our eyes. This is a known problem — unrestricted pixel optimization produces adversarial-looking images.\n",
    "\n",
    "Lucent solves this with two key tricks:\n",
    "\n",
    "- **FFT parameterization:** Represent the image in the frequency domain. This naturally penalizes high frequencies, so the optimizer produces smoother, more natural patterns.\n",
    "\n",
    "- **Color decorrelation:** Optimize in a decorrelated color space (based on natural image statistics). This lets the optimizer explore realistic color combinations instead of getting stuck in unnatural hues.\n",
    "\n",
    "Let's see the difference — same neuron, same number of steps, three levels of sophistication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Same neuron, three approaches ----------\n\n",
    "target = objectives.channel(LAYER, TARGET_NEURON)\n\n",
    "\n\n",
    "# (A) Lucent with naive pixel parameterization (no tricks)\n\n",
    "imgs_naive = render.render_vis(\n\n",
    "    model, target,\n\n",
    "    param_f=lambda: param.image(128, fft=False, decorrelate=False),\n\n",
    "    transforms=[],\n\n",
    "    thresholds=(512,),\n\n",
    "    show_inline=False, show_image=False,\n\n",
    ")\n\n",
    "\n\n",
    "# (B) Lucent with full pipeline (FFT + decorrelation + standard transforms)\n\n",
    "imgs_full = render.render_vis(\n\n",
    "    model, target,\n\n",
    "    param_f=lambda: param.image(128, fft=True, decorrelate=True),\n\n",
    "    transforms=transform.standard_transforms,\n\n",
    "    thresholds=(512,),\n\n",
    "    show_inline=False, show_image=False,\n\n",
    ")\n\n",
    "\n\n",
    "# --- Three-panel comparison ---\n\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n",
    "\n\n",
    "# Panel 1: Our DIY result from the previous cell\n\n",
    "diy_final = snapshots[-1][1]\n\n",
    "axes[0].imshow(diy_final)\n\n",
    "axes[0].set_title(\"Our DIY loop\\n(raw pixels, no tricks)\", fontsize=11)\n\n",
    "\n\n",
    "# Panel 2: Lucent without tricks\n\n",
    "axes[1].imshow(imgs_naive[0][0])\n\n",
    "axes[1].set_title(\"Lucent — naive pixels\\n(fft=False, decorrelate=False)\", fontsize=11)\n\n",
    "\n\n",
    "# Panel 3: Lucent with full pipeline\n\n",
    "axes[2].imshow(imgs_full[0][0])\n\n",
    "axes[2].set_title(\"Lucent — full pipeline\\n(fft + decorrelation + transforms)\", fontsize=11)\n\n",
    "\n\n",
    "for ax in axes:\n\n",
    "    ax.axis(\"off\")\n\n",
    "\n\n",
    "fig.suptitle(\n\n",
    "    f\"mixed4a neuron {TARGET_NEURON}: parameterization turns noise into clarity\",\n\n",
    "    fontsize=13, fontweight=\"bold\",\n\n",
    ")\n\n",
    "plt.tight_layout()\n\n",
    "plt.show()\n\n",
    "\n\n",
    "print(\"Same neuron, same optimization objective — the only difference is how we represent the image.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## What we know so far\n\n| Step | What we saw |\n|------|------------|\n| **Gallery** | 10 neurons in mixed4a each learned to detect a unique visual pattern |\n| **DIY loop** | Gradient ascent on pixels works, but raw optimization produces noise |\n| **Parameterization** | FFT + decorrelation + transforms produce clean, interpretable results |\n\nOne question remains: **does this only work for mixed4a?**\n\nSegment 1 showed us that CNNs build a feature hierarchy — early layers detect edges, deeper layers detect objects. Let's apply activation maximization across multiple layers and neurons to see if that hierarchy shows up here too."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Summary figure: 3 neurons across 3 layers ----------\n\n",
    "# Rows = different neurons, Columns = increasing network depth\n\n",
    "\n\n",
    "poster_layers = [\"mixed3a\", \"mixed4a\", \"mixed5b\"]\n\n",
    "poster_neurons = [0, 5, 9]\n\n",
    "col_labels = [\"Early-mid (mixed3a)\", \"Mid (mixed4a)\", \"Late (mixed5b)\"]\n\n",
    "\n\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n\n",
    "\n\n",
    "for row, neuron_idx in enumerate(poster_neurons):\n\n",
    "    for col, lyr in enumerate(poster_layers):\n\n",
    "        print(f\"  {lyr} neuron {neuron_idx}...\", end=\" \", flush=True)\n\n",
    "        imgs = render.render_vis(\n\n",
    "            model,\n\n",
    "            objectives.channel(lyr, neuron_idx),\n\n",
    "            param_f=lambda: param.image(128, fft=True, decorrelate=True),\n\n",
    "            thresholds=(512,),\n\n",
    "            show_inline=False, show_image=False,\n\n",
    "        )\n\n",
    "        axes[row, col].imshow(imgs[0][0])\n\n",
    "        axes[row, col].axis(\"off\")\n\n",
    "\n\n",
    "        if row == 0:\n\n",
    "            axes[row, col].set_title(col_labels[col], fontsize=11, fontweight=\"bold\")\n\n",
    "\n\n",
    "    axes[row, 0].set_ylabel(\n\n",
    "        f\"Neuron {neuron_idx}\", fontsize=11, fontweight=\"bold\",\n\n",
    "        rotation=0, labelpad=55, va=\"center\",\n\n",
    "    )\n\n",
    "\n\n",
    "print(\"\\ndone\")\n\n",
    "\n\n",
    "fig.suptitle(\n\n",
    "    \"Features grow more complex with depth — the hierarchy Segment 1 predicted\",\n\n",
    "    fontsize=13, fontweight=\"bold\", y=1.01,\n\n",
    ")\n\n",
    "plt.tight_layout()\n\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | What we learned |\n",
    "|---------|----------------|\n",
    "| **Activation maximization** | Generate images that maximally activate a specific neuron — revealing the pattern it has learned to detect |\n",
    "| **Gradient ascent on pixels** | Freeze the network, optimize the input — the reverse of training |\n",
    "| **Parameterization matters** | FFT + color decorrelation + transforms turn noise into interpretable images |\n",
    "| **Feature hierarchy** | Early layers dream of edges, mid layers of textures, deep layers of object parts |\n",
    "\n",
    "### Limitations to keep in mind\n",
    "\n",
    "- These images show what **maximally** excites a neuron, not the full range of inputs it responds to in practice.\n",
    "- Some neurons are **polysemantic** — they respond to multiple unrelated patterns. A single activation-maximized image cannot reveal this.\n",
    "- The parameterization choices (FFT, decorrelation) act as a **prior** on what kinds of images we allow. Different priors can yield different-looking results.\n",
    "\n",
    "---\n",
    "\n",
    "### What's next\n",
    "\n",
    "We now know what neurons *want* to see (activation maximization). The natural follow-up is to ask: **what do they *actually* see in real images?** Finding the real-world image patches that most strongly activate specific neurons connects these synthetic dreams back to the visual world — and that's where the story continues."
   ]
  }
 ]
}