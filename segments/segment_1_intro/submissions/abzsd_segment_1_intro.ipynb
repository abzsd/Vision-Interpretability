{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment 1: Introduction to Vision Interpretability and CNN Basics\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](#)\n",
    "\n",
    "**What you'll learn:**\n",
    "1. How images become numbers (tensors) that a network can process\n",
    "2. What convolutional filters do \u2014 local pattern matching\n",
    "3. How to build and train a simple CNN from scratch\n",
    "4. How depth creates abstraction: edges \u2192 textures \u2192 objects\n",
    "5. How spatial information survives through the network's layers\n",
    "\n",
    "We'll keep things visual and hands-on. By the end, you'll have trained your own CNN and looked inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n\n",
    "import torch.nn as nn\n\n",
    "import torch.nn.functional as F\n\n",
    "import torch.optim as optim\n\n",
    "from torchvision import transforms, datasets, models\n\n",
    "from torch.utils.data import DataLoader\n\n",
    "from PIL import Image\n\n",
    "import matplotlib.pyplot as plt\n\n",
    "import numpy as np\n\n",
    "import requests, json, os, tarfile, urllib.request\n\n",
    "from io import BytesIO\n\n",
    "\n\n",
    "# Device setup\n\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n",
    "print(f\"Using device: {device}\")\n\n",
    "\n\n",
    "# Download a sample image\n\n",
    "url = \"https://raw.githubusercontent.com/pytorch/hub/master/images/dog.jpg\"\n\n",
    "img = Image.open(BytesIO(requests.get(url).content)).convert(\"RGB\").resize((224, 224))\n\n",
    "\n\n",
    "# Convert to tensor (no normalization \u2014 just raw pixel values scaled to [0, 1])\n\n",
    "img_tensor = transforms.ToTensor()(img)  # shape: [3, 224, 224]\n\n",
    "\n\n",
    "print(f\"Image tensor shape: {list(img_tensor.shape)}  \u2192  [channels, height, width]\")\n\n",
    "print(f\"Value range: [{img_tensor.min():.2f}, {img_tensor.max():.2f}]\")\n\n",
    "\n\n",
    "# --- Human view vs. Machine view ---\n\n",
    "fig, axes = plt.subplots(1, 4, figsize=(14, 3.5))\n\n",
    "axes[0].imshow(img)\n\n",
    "axes[0].set_title(\"What we see\")\n\n",
    "\n\n",
    "for c, (name, cmap) in enumerate([(\"Red\", \"Reds\"), (\"Green\", \"Greens\"), (\"Blue\", \"Blues\")]):\n\n",
    "    axes[c + 1].imshow(img_tensor[c].numpy(), cmap=cmap)\n\n",
    "    axes[c + 1].set_title(f\"{name} channel\")\n\n",
    "\n\n",
    "for ax in axes:\n\n",
    "    ax.axis(\"off\")\n\n",
    "\n\n",
    "plt.suptitle(\"To a CNN, an image is just a 3D grid of numbers \u2014 one layer per color channel\",\n\n",
    "             fontsize=11, y=1.02)\n\n",
    "plt.tight_layout()\n\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Does a Convolutional Filter Do?\n",
    "\n",
    "A **filter** (or kernel) is a small grid of numbers, typically 3\u00d73. It slides across the image one patch at a time and computes a weighted sum at each position:\n",
    "\n",
    "- If the patch **matches** the filter's pattern \u2192 **high output** (bright)\n",
    "- If it **doesn't match** \u2192 **low output** (dark)\n",
    "\n",
    "This is the core mechanism of every CNN: **local pattern matching via dot products.**\n",
    "\n",
    "Let's make this concrete with a toy experiment \u2014 two simple images, three hand-crafted filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Two simple test images (10x10 pixels) ---\n\n",
    "img_vert = torch.zeros(10, 10)\n\n",
    "img_vert[:, 5:] = 1.0                  # vertical edge: left half dark, right half bright\n\n",
    "\n\n",
    "img_diag = torch.zeros(10, 10)\n\n",
    "for i in range(10):\n\n",
    "    img_diag[i, min(i, 9)] = 1.0       # diagonal line\n\n",
    "\n\n",
    "# --- Three hand-crafted 3x3 filters ---\n\n",
    "filters = {\n\n",
    "    \"Vertical\\ndetector\": torch.tensor([[-1., 0., 1.],\n\n",
    "                                         [-1., 0., 1.],\n\n",
    "                                         [-1., 0., 1.]]),\n\n",
    "    \"Horizontal\\ndetector\": torch.tensor([[-1., -1., -1.],\n\n",
    "                                           [ 0.,  0.,  0.],\n\n",
    "                                           [ 1.,  1.,  1.]]),\n\n",
    "    \"Diagonal\\ndetector\": torch.tensor([[ 2., -1., -1.],\n\n",
    "                                         [-1.,  2., -1.],\n\n",
    "                                         [-1., -1.,  2.]]),\n\n",
    "}\n\n",
    "\n\n",
    "# --- Apply every filter to every image ---\n\n",
    "test_images = {\"Vertical edge\": img_vert, \"Diagonal line\": img_diag}\n\n",
    "\n\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 6))\n\n",
    "for r, (img_name, im) in enumerate(test_images.items()):\n\n",
    "    axes[r, 0].imshow(im, cmap=\"gray\")\n\n",
    "    axes[r, 0].set_title(img_name, fontsize=11)\n\n",
    "\n\n",
    "    x = im.unsqueeze(0).unsqueeze(0)  # reshape to [1, 1, 10, 10] for conv2d\n\n",
    "    for c, (f_name, kernel) in enumerate(filters.items()):\n\n",
    "        out = F.conv2d(x, kernel.reshape(1, 1, 3, 3), padding=1)\n\n",
    "        axes[r, c + 1].imshow(out.squeeze(), cmap=\"RdBu_r\", vmin=-3, vmax=3)\n\n",
    "        axes[r, c + 1].set_title(f_name, fontsize=10)\n\n",
    "\n\n",
    "for ax in axes.flat:\n\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n\n",
    "\n\n",
    "plt.suptitle(\"Filter selectivity: each filter responds strongest to its own pattern\",\n\n",
    "             fontsize=12, fontweight=\"bold\")\n\n",
    "plt.tight_layout()\n\n",
    "plt.show()\n\n",
    "\n\n",
    "# Key insight:\n\n",
    "print(\"Each filter is a pattern template \u2014 it only fires when it sees its matching pattern.\")\n\n",
    "print(\"A CNN learns HUNDREDS of these filters automatically during training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Build and Train a Tiny CNN\n",
    "\n",
    "Now that we understand what a single filter does, let's build a network with **many filters stacked in layers** and train it to recognize images.\n",
    "\n",
    "**Dataset:** [ImageNette](https://github.com/fastai/imagenette) \u2014 a beginner-friendly 10-class subset of ImageNet with easy-to-recognize categories (dog, church, guitar, fish, etc.)\n",
    "\n",
    "**Our model \u2014 SimpleCNN:**\n",
    "- `conv1`: 3 \u2192 16 filters (learn basic patterns like edges)\n",
    "- `conv2`: 16 \u2192 32 filters (combine edges into textures)\n",
    "- `conv3`: 32 \u2192 64 filters (build higher-level features)\n",
    "- Global average pool + fully connected layer \u2192 10 class scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n\n",
    "# SimpleCNN \u2014 a minimal 3-layer convolutional network\n\n",
    "# ===========================================================\n\n",
    "class SimpleCNN(nn.Module):\n\n",
    "    def __init__(self, num_classes=10):\n\n",
    "        super().__init__()\n\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n\n",
    "        self.bn1   = nn.BatchNorm2d(16)\n\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n\n",
    "        self.bn2   = nn.BatchNorm2d(32)\n\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n\n",
    "        self.bn3   = nn.BatchNorm2d(64)\n\n",
    "        self.pool  = nn.MaxPool2d(2, 2)\n\n",
    "        self.gap   = nn.AdaptiveAvgPool2d(1)   # global average pool\n\n",
    "        self.fc    = nn.Linear(64, num_classes)\n\n",
    "\n\n",
    "    def forward(self, x):\n\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))   # [B,16,H/2,W/2]\n\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))   # [B,32,H/4,W/4]\n\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))   # [B,64,H/8,W/8]\n\n",
    "        x = self.gap(x).flatten(1)                        # [B,64]\n\n",
    "        return self.fc(x)                                  # [B,10]\n\n",
    "\n\n",
    "# ===========================================================\n\n",
    "# Download and load ImageNette (160px version, ~98 MB)\n\n",
    "# ===========================================================\n\n",
    "DATA_DIR = \"imagenette2-160\"\n\n",
    "if not os.path.exists(DATA_DIR):\n\n",
    "    print(\"Downloading ImageNette (160px)... this may take a minute.\")\n\n",
    "    urllib.request.urlretrieve(\n\n",
    "        \"https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz\",\n\n",
    "        \"imagenette2-160.tgz\"\n\n",
    "    )\n\n",
    "    with tarfile.open(\"imagenette2-160.tgz\", \"r:gz\") as tar:\n\n",
    "        tar.extractall()\n\n",
    "    print(\"Done!\")\n\n",
    "\n\n",
    "# ImageNet normalization stats\n\n",
    "MEAN = [0.485, 0.456, 0.406]\n\n",
    "STD  = [0.229, 0.224, 0.225]\n\n",
    "\n\n",
    "train_transform = transforms.Compose([\n\n",
    "    transforms.RandomResizedCrop(128),\n\n",
    "    transforms.RandomHorizontalFlip(),\n\n",
    "    transforms.ToTensor(),\n\n",
    "    transforms.Normalize(mean=MEAN, std=STD),\n\n",
    "])\n\n",
    "val_transform = transforms.Compose([\n\n",
    "    transforms.Resize(160),\n\n",
    "    transforms.CenterCrop(128),\n\n",
    "    transforms.ToTensor(),\n\n",
    "    transforms.Normalize(mean=MEAN, std=STD),\n\n",
    "])\n\n",
    "\n\n",
    "train_dataset = datasets.ImageFolder(f\"{DATA_DIR}/train\", transform=train_transform)\n\n",
    "val_dataset   = datasets.ImageFolder(f\"{DATA_DIR}/val\",   transform=val_transform)\n\n",
    "\n\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True,  num_workers=2)\n\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=64, shuffle=False, num_workers=2)\n\n",
    "\n\n",
    "CLASS_NAMES = [\"tench\", \"English springer\", \"cassette player\", \"chain saw\",\n\n",
    "               \"church\", \"French horn\", \"garbage truck\", \"gas pump\",\n\n",
    "               \"golf ball\", \"parachute\"]\n\n",
    "\n\n",
    "print(f\"Training samples:   {len(train_dataset)}\")\n\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n\n",
    "print(f\"Classes: {CLASS_NAMES}\")\n\n",
    "\n\n",
    "# --- Show a sample batch ---\n\n",
    "mean_t = torch.tensor(MEAN).view(3, 1, 1)\n\n",
    "std_t  = torch.tensor(STD).view(3, 1, 1)\n\n",
    "\n\n",
    "sample_imgs, sample_labels = next(iter(train_loader))\n\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n\n",
    "for i, ax in enumerate(axes.flat):\n\n",
    "    display = (sample_imgs[i] * std_t + mean_t).permute(1, 2, 0).clip(0, 1)\n\n",
    "    ax.imshow(display)\n\n",
    "    ax.set_title(CLASS_NAMES[sample_labels[i]], fontsize=10)\n\n",
    "    ax.axis(\"off\")\n\n",
    "plt.suptitle(\"Sample training images from ImageNette\", fontsize=12, fontweight=\"bold\")\n\n",
    "plt.tight_layout()\n\n",
    "plt.show()\n\n",
    "\n\n",
    "# --- Create model ---\n\n",
    "model_simple = SimpleCNN(num_classes=10).to(device)\n\n",
    "total_params = sum(p.numel() for p in model_simple.parameters())\n\n",
    "print(f\"\\nSimpleCNN created on {device}: {total_params:,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n\n",
    "# Training loop\n\n",
    "# ===========================================================\n\n",
    "optimizer = optim.Adam(model_simple.parameters(), lr=1e-3)\n\n",
    "criterion = nn.CrossEntropyLoss()\n\n",
    "num_epochs = 8\n\n",
    "\n\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n\n",
    "\n\n",
    "for epoch in range(num_epochs):\n\n",
    "    # --- Training pass ---\n\n",
    "    model_simple.train()\n\n",
    "    running_loss = 0.0\n\n",
    "    for images, labels in train_loader:\n\n",
    "        images, labels = images.to(device), labels.to(device)\n\n",
    "        optimizer.zero_grad()\n\n",
    "        loss = criterion(model_simple(images), labels)\n\n",
    "        loss.backward()\n\n",
    "        optimizer.step()\n\n",
    "        running_loss += loss.item() * images.size(0)\n\n",
    "\n\n",
    "    train_loss = running_loss / len(train_dataset)\n\n",
    "\n\n",
    "    # --- Validation pass ---\n\n",
    "    model_simple.eval()\n\n",
    "    val_loss, correct = 0.0, 0\n\n",
    "    with torch.no_grad():\n\n",
    "        for images, labels in val_loader:\n\n",
    "            images, labels = images.to(device), labels.to(device)\n\n",
    "            outputs = model_simple(images)\n\n",
    "            val_loss += criterion(outputs, labels).item() * images.size(0)\n\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n\n",
    "\n\n",
    "    val_loss /= len(val_dataset)\n\n",
    "    val_acc = correct / len(val_dataset) * 100\n\n",
    "\n\n",
    "    history[\"train_loss\"].append(train_loss)\n\n",
    "    history[\"val_loss\"].append(val_loss)\n\n",
    "    history[\"val_acc\"].append(val_acc)\n\n",
    "\n\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}  |  \"\n\n",
    "          f\"Train loss: {train_loss:.3f}  |  \"\n\n",
    "          f\"Val loss: {val_loss:.3f}  |  Val acc: {val_acc:.1f}%\")\n\n",
    "\n\n",
    "# --- Plot training curves ---\n\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n",
    "ax1.plot(history[\"train_loss\"], label=\"Train\", marker=\"o\")\n\n",
    "ax1.plot(history[\"val_loss\"], label=\"Validation\", marker=\"o\")\n\n",
    "ax1.set_xlabel(\"Epoch\"); ax1.set_ylabel(\"Loss\"); ax1.legend()\n\n",
    "ax1.set_title(\"Loss\")\n\n",
    "\n\n",
    "ax2.plot(history[\"val_acc\"], color=\"green\", marker=\"o\")\n\n",
    "ax2.set_xlabel(\"Epoch\"); ax2.set_ylabel(\"Accuracy (%)\")\n\n",
    "ax2.set_title(\"Validation Accuracy\")\n\n",
    "\n\n",
    "plt.tight_layout()\n\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Did Our CNN Learn?\n",
    "\n",
    "Our SimpleCNN now classifies 10 categories of images. But what patterns did it actually discover?\n",
    "\n",
    "The first layer (`conv1`) has **16 filters**, each 3\u00d73\u00d73 (height \u00d7 width \u00d7 RGB channels). Before training these were random noise. After training, they should have organized into meaningful pattern detectors.\n",
    "\n",
    "Let's compare before vs. after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained filters from our model\n\n",
    "trained_filters = model_simple.conv1.weight.data.cpu()   # [16, 3, 3, 3]\n\n",
    "\n\n",
    "# Random filters from a fresh (untrained) model for comparison\n\n",
    "random_filters = SimpleCNN().conv1.weight.data.cpu()\n\n",
    "\n\n",
    "def show_filter_row(filters, axes_row):\n\n",
    "    \"\"\"Display filters as tiny RGB images in a row of axes.\"\"\"\n\n",
    "    for idx, ax in enumerate(axes_row):\n\n",
    "        if idx < filters.shape[0]:\n\n",
    "            f = filters[idx].permute(1, 2, 0).numpy()        # [3,3,RGB]\n\n",
    "            f = (f - f.min()) / (f.max() - f.min() + 1e-8)   # normalize to [0,1]\n\n",
    "            ax.imshow(f, interpolation=\"nearest\")\n\n",
    "        ax.axis(\"off\")\n\n",
    "\n\n",
    "fig, axes = plt.subplots(2, 16, figsize=(16, 2.5))\n\n",
    "show_filter_row(random_filters, axes[0])\n\n",
    "show_filter_row(trained_filters, axes[1])\n\n",
    "\n\n",
    "axes[0][0].set_ylabel(\"Random\\n(before)\", fontsize=10, rotation=0, labelpad=50, va=\"center\")\n\n",
    "axes[1][0].set_ylabel(\"Learned\\n(after)\",  fontsize=10, rotation=0, labelpad=50, va=\"center\")\n\n",
    "\n\n",
    "plt.suptitle(\"conv1 filters: random noise \u2192 learned edge and color detectors\",\n\n",
    "             fontsize=12, fontweight=\"bold\")\n\n",
    "plt.tight_layout()\n\n",
    "plt.show()\n\n",
    "\n\n",
    "print(\"The network discovered edge and color detectors on its own \u2014 nobody programmed them.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hook activations at each convolutional layer ---\n\n",
    "activations = {}\n\n",
    "\n\n",
    "def get_hook(name):\n\n",
    "    def hook_fn(module, input, output):\n\n",
    "        activations[name] = output.detach().cpu()\n\n",
    "    return hook_fn\n\n",
    "\n\n",
    "hooks = [\n\n",
    "    model_simple.bn1.register_forward_hook(get_hook(\"conv1 (16ch)\")),\n\n",
    "    model_simple.bn2.register_forward_hook(get_hook(\"conv2 (32ch)\")),\n\n",
    "    model_simple.bn3.register_forward_hook(get_hook(\"conv3 (64ch)\")),\n\n",
    "]\n\n",
    "\n\n",
    "# Run our sample image through the trained SimpleCNN\n\n",
    "sample_input = val_transform(img).unsqueeze(0).to(device)\n\n",
    "model_simple.eval()\n\n",
    "with torch.no_grad():\n\n",
    "    _ = model_simple(sample_input)\n\n",
    "\n\n",
    "for h in hooks:\n\n",
    "    h.remove()\n\n",
    "\n\n",
    "# --- Mean activation at each depth ---\n\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n\n",
    "axes[0].imshow(img)\n\n",
    "axes[0].set_title(\"Input image\", fontsize=11)\n\n",
    "\n\n",
    "for i, (name, act) in enumerate(activations.items()):\n\n",
    "    # Apply ReLU (hooks captured pre-ReLU output from BN)\n\n",
    "    mean_act = F.relu(act[0]).mean(dim=0)\n\n",
    "    C, H, W = act.shape[1], act.shape[2], act.shape[3]\n\n",
    "    axes[i + 1].imshow(mean_act, cmap=\"inferno\")\n\n",
    "    axes[i + 1].set_title(f\"{name}\\n{C} \u00d7 {H}\u00d7{W}\", fontsize=10)\n\n",
    "\n\n",
    "for ax in axes:\n\n",
    "    ax.axis(\"off\")\n\n",
    "plt.suptitle(\"Deeper layers \u2192 more abstract features, lower spatial resolution\",\n\n",
    "             fontsize=12, fontweight=\"bold\")\n\n",
    "plt.tight_layout()\n\n",
    "plt.show()\n\n",
    "\n\n",
    "# --- Individual feature maps from conv1 ---\n\n",
    "conv1_act = F.relu(activations[\"conv1 (16ch)\"][0])   # [16, H, W]\n\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n\n",
    "for i, ax in enumerate(axes.flat):\n\n",
    "    fmap = conv1_act[i]\n\n",
    "    fmap = (fmap - fmap.min()) / (fmap.max() - fmap.min() + 1e-5)\n\n",
    "    ax.imshow(fmap, cmap=\"viridis\")\n\n",
    "    ax.set_title(f\"#{i}\", fontsize=8)\n\n",
    "    ax.axis(\"off\")\n\n",
    "plt.suptitle(\"All 16 feature maps from conv1 \u2014 each filter highlights a different pattern\",\n\n",
    "             fontsize=11, fontweight=\"bold\")\n\n",
    "plt.tight_layout()\n\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does the Network Know *Where* Things Are?\n",
    "\n",
    "Deeper layers have smaller spatial dimensions \u2014 the feature maps physically shrink. Does that mean the network loses track of where things are in the image?\n",
    "\n",
    "**Experiment:** black out a region of the input and see which activations change. If spatial information is preserved, only the *corresponding region* of the feature maps should be disrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a perturbed copy with a blacked-out region\n\n",
    "perturbed_input = sample_input.clone()\n\n",
    "perturbed_input[:, :, 30:90, 40:110] = 0   # zero out a rectangular patch\n\n",
    "\n\n",
    "def collect_acts(x):\n\n",
    "    \"\"\"Run x through the model and return early + deep activations.\"\"\"\n\n",
    "    store = {}\n\n",
    "    def hook_early(m, inp, out): store[\"early\"] = out.detach().cpu()\n\n",
    "    def hook_deep(m, inp, out):  store[\"deep\"]  = out.detach().cpu()\n\n",
    "\n\n",
    "    h1 = model_simple.bn1.register_forward_hook(hook_early)\n\n",
    "    h2 = model_simple.bn3.register_forward_hook(hook_deep)\n\n",
    "    model_simple.eval()\n\n",
    "    with torch.no_grad():\n\n",
    "        model_simple(x)\n\n",
    "    h1.remove(); h2.remove()\n\n",
    "    return store\n\n",
    "\n\n",
    "acts_orig = collect_acts(sample_input)\n\n",
    "acts_pert = collect_acts(perturbed_input)\n\n",
    "\n\n",
    "# Absolute difference in activations (averaged across channels)\n\n",
    "early_diff = (acts_orig[\"early\"][0] - acts_pert[\"early\"][0]).abs().mean(0)\n\n",
    "deep_diff  = (acts_orig[\"deep\"][0]  - acts_pert[\"deep\"][0]).abs().mean(0)\n\n",
    "\n\n",
    "# --- Visualize ---\n\n",
    "mean_t = torch.tensor(MEAN).view(3, 1, 1)\n\n",
    "std_t  = torch.tensor(STD).view(3, 1, 1)\n\n",
    "\n\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n\n",
    "\n\n",
    "axes[0].imshow(img)\n\n",
    "axes[0].set_title(\"Original\")\n\n",
    "\n\n",
    "pert_display = (perturbed_input[0].cpu() * std_t + mean_t).permute(1, 2, 0).clip(0, 1)\n\n",
    "axes[1].imshow(pert_display)\n\n",
    "axes[1].set_title(\"Perturbed (blacked out)\")\n\n",
    "\n\n",
    "axes[2].imshow(early_diff, cmap=\"hot\")\n\n",
    "axes[2].set_title(f\"conv1 difference\\n{list(early_diff.shape)}\")\n\n",
    "\n\n",
    "axes[3].imshow(deep_diff, cmap=\"hot\")\n\n",
    "axes[3].set_title(f\"conv3 difference\\n{list(deep_diff.shape)}\")\n\n",
    "\n\n",
    "for ax in axes:\n\n",
    "    ax.axis(\"off\")\n\n",
    "plt.suptitle(\"Spatial information survives: the disruption appears in the right location, even in deep layers\",\n\n",
    "             fontsize=11, fontweight=\"bold\")\n\n",
    "plt.tight_layout()\n\n",
    "plt.show()\n\n",
    "\n\n",
    "print(\"Even at low resolution, deep layers preserve WHERE things are in the image.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Up: A Quick Look at Pretrained Models\n",
    "\n",
    "Our SimpleCNN has 3 layers and learned from just 10 classes. State-of-the-art models have **dozens of layers** and are trained on **millions of images** from the full ImageNet (1000+ classes).\n",
    "\n",
    "The same principles apply at scale \u2014 early layers detect edges, deeper layers detect objects. The features are just much richer.\n",
    "\n",
    "Popular architectures include **ResNet**, **VGG**, and **GoogLeNet (InceptionV1)** \u2014 which we'll use in Segment 2.\n",
    "\n",
    "Let's quickly load a pretrained ResNet18 and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load pretrained ResNet18 ---\n\n",
    "resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT).to(device).eval()\n\n",
    "resnet_preprocess = models.ResNet18_Weights.DEFAULT.transforms()\n\n",
    "\n\n",
    "# --- Classify our sample image ---\n\n",
    "resnet_input = resnet_preprocess(img).unsqueeze(0).to(device)\n\n",
    "with torch.no_grad():\n\n",
    "    logits = resnet(resnet_input)\n\n",
    "\n\n",
    "# Load human-readable ImageNet labels\n\n",
    "labels_url = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n\n",
    "imagenet_labels = json.loads(requests.get(labels_url).text)\n\n",
    "\n\n",
    "probs = torch.softmax(logits, dim=1)[0]\n\n",
    "top5 = probs.topk(5)\n\n",
    "\n\n",
    "print(\"ResNet18 top-5 predictions:\")\n\n",
    "for i in range(5):\n\n",
    "    idx = top5.indices[i].item()\n\n",
    "    print(f\"  {imagenet_labels[idx]:25s}  {top5.values[i]:.1%}\")\n\n",
    "\n\n",
    "# --- Feature maps at two depths ---\n\n",
    "resnet_acts = {}\n\n",
    "\n\n",
    "def resnet_hook(name):\n\n",
    "    def fn(m, inp, out):\n\n",
    "        resnet_acts[name] = out.detach().cpu()\n\n",
    "    return fn\n\n",
    "\n\n",
    "hooks = [\n\n",
    "    resnet.layer1.register_forward_hook(resnet_hook(\"layer1 (early)\")),\n\n",
    "    resnet.layer4.register_forward_hook(resnet_hook(\"layer4 (deep)\")),\n\n",
    "]\n\n",
    "with torch.no_grad():\n\n",
    "    _ = resnet(resnet_input)\n\n",
    "for h in hooks:\n\n",
    "    h.remove()\n\n",
    "\n\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n\n",
    "axes[0].imshow(img)\n\n",
    "axes[0].set_title(\"Input image\")\n\n",
    "\n\n",
    "for i, (name, act) in enumerate(resnet_acts.items()):\n\n",
    "    mean_act = act[0].mean(dim=0)\n\n",
    "    C, H, W = act.shape[1], act.shape[2], act.shape[3]\n\n",
    "    axes[i + 1].imshow(mean_act, cmap=\"inferno\")\n\n",
    "    axes[i + 1].set_title(f\"ResNet18 {name}\\n{C}ch \u00d7 {H}\u00d7{W}\")\n\n",
    "\n\n",
    "for ax in axes:\n\n",
    "    ax.axis(\"off\")\n\n",
    "plt.suptitle(\"Same principles at scale: early layers see edges, deep layers see objects\",\n\n",
    "             fontsize=12, fontweight=\"bold\")\n\n",
    "plt.tight_layout()\n\n",
    "plt.show()\n\n",
    "\n\n",
    "print(\"A pretrained model has much richer features \u2014 but the same hierarchy holds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | Key Takeaway |\n",
    "|---------|-------------|\n",
    "| **Images as tensors** | A CNN sees a [3, H, W] grid of numbers, not a photograph |\n",
    "| **Filter selectivity** | Each filter is a pattern template \u2014 it only fires on matching patterns |\n",
    "| **Learned features** | Training discovers meaningful filters (edges, colors) automatically |\n",
    "| **Feature hierarchy** | Early layers = edges, middle = textures, deep = whole objects |\n",
    "| **Spatial information** | Position is preserved even in deep, low-resolution layers |\n",
    "\n",
    "---\n",
    "\n",
    "### Next Up: Segment 2 \u2014 Activation Maximization\n",
    "\n",
    "So far we've asked: *\"What does each neuron do when it sees our image?\"*\n",
    "\n",
    "In Segment 2, we'll flip the question: **\"What input image would make a given neuron fire the hardest?\"**\n",
    "\n",
    "That's the idea behind **activation maximization** \u2014 and we'll use the [Lucent](https://github.com/greentfrapp/lucent) library with **InceptionV1 (GoogLeNet)** to generate these visualizations for the first 10 neurons of the **Mixed4a** layer.\n",
    "\n",
    "Stay tuned!"
   ]
  }
 ]
}